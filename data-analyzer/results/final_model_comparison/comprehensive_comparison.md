# 完整模型比較報告

**生成時間**: 2025-12-27 15:31:32

## 1. 模型概覽

| 模型 | 特徵數量 | 配置 |
|------|---------|------|
| XGBoost | 84 | n_estimators=200, max_depth=8, lr=0.05 |
| LSTM (簡化) | 6 | hidden=32, layers=1, seq=30 |
| LSTM (完整) | 118 | hidden=64, layers=2, seq=30, dropout=0.2 |

## 2. 性能比較

### 2.1 關鍵指標

| 指標 | XGBoost | LSTM (簡化) | LSTM (完整) | 最佳模型 |
|------|---------|------------|------------|---------|
| **RMSE** | 253.96 | 662.59 | 693.17 | **XGBoost** |
| **MAE** | 179.85 | 505.44 | 515.70 | **XGBoost** |
| **R²** | 0.999270 | 0.988256 | 0.987147 | **XGBoost** |
| **MAPE** | 0.185% | 0.550% | 0.558% | **XGBoost** |
| **方向準確率** | 86.43% | 47.41% | 46.25% | **XGBoost** |

### 2.2 性能差距分析

**相對於 XGBoost 的性能差距：**

#### LSTM (簡化版, 6 特徵)
- RMSE: 2.61x 更差
- MAE: 2.81x 更差
- 方向準確率: 0.55x 更差

#### LSTM (完整版, 118 特徵)
- RMSE: 2.73x 更差
- MAE: 2.87x 更差
- 方向準確率: 0.54x 更差

### 2.3 LSTM 版本比較

**簡化版 vs 完整版：**
- RMSE: 簡化版更好 (30.58 USDT 差距)
- MAE: 簡化版更好 (10.26 USDT 差距)
- R²: 簡化版更好 (0.001109 差距)
- 方向準確率: 簡化版更好 (1.16% 差距)

## 3. 深度分析

### 3.1 為什麼 XGBoost 顯著優於 LSTM？

1. **特徵處理方式**：
   - XGBoost 直接處理表格型特徵，適合時序數據的統計特徵（lag, rolling, 技術指標）
   - LSTM 需要序列結構，可能丟失了一些跨時間步的複雜模式

2. **方向預測能力**：
   - XGBoost: 86.43%（遠超隨機）
   - LSTM: ~47%（接近隨機）
   - 這表明 XGBoost 更好地捕捉了價格變化趨勢

3. **訓練穩定性**：
   - XGBoost: 梯度提升，穩定且可解釋
   - LSTM: 需要仔細調參，容易陷入局部最優

### 3.2 為什麼增加 LSTM 特徵反而沒有提升性能？

**簡化版（6 特徵）vs 完整版（118 特徵）：**

| 指標 | 簡化版 | 完整版 | 結論 |
|------|-------|-------|------|
| RMSE | 662.59 | 693.17 | 簡化版更好 |
| 方向準確率 | 47.41% | 46.25% | 簡化版更好 |

**可能原因：**

1. **維度災難**：118 個特徵可能導致 LSTM 過擬合訓練集
2. **訓練不足**：更多特徵需要更多訓練數據和更長訓練時間
3. **特徵冗餘**：很多特徵高度相關，增加噪音而非信號
4. **序列長度**：30 步序列 × 118 特徵 = 3,540 個輸入，可能過於複雜

### 3.3 LSTM 改進建議

1. **特徵選擇**：
   - 使用 XGBoost 特徵重要性篩選最重要的 20-30 個特徵
   - 進行主成分分析（PCA）降維

2. **架構調整**：
   - 嘗試 Bidirectional LSTM
   - 使用 Attention 機制
   - 探索 Transformer 架構

3. **數據預處理**：
   - 使用價格差分而非絕對價格
   - 添加更多數據（目前只有 1 年）
   - 考慮多變量時間序列建模

4. **訓練策略**：
   - 更長的訓練時間
   - 更細緻的超參數搜索
   - 使用交叉驗證

## 4. 最終結論與建議

### ✅ 明確推薦：使用 XGBoost

**理由：**

1. **預測精度最高**：
   - RMSE 低於 LSTM 2.6-2.7 倍
   - R² 達到 0.9993（幾乎完美）

2. **方向預測最準**：
   - 86.43% 準確率，適合交易策略
   - LSTM 方向準確率接近隨機（~47%）

3. **實用性最強**：
   - 訓練快速（相比深度學習）
   - 可解釋性強（特徵重要性）
   - 生產部署簡單

4. **穩定性最佳**：
   - 超參數敏感度低
   - 不容易過擬合

### ⚠️ LSTM 不推薦用於此任務

**關鍵問題：**

1. 方向準確率僅 47%，無法用於交易決策
2. 增加特徵未能提升性能，反而略有下降
3. 訓練時間長，調參困難

**可能適用場景：**

- 更長的序列（日線、週線數據）
- 多變量時間序列（結合訂單簿、鏈上數據）
- 與 XGBoost 集成學習

### 📊 數據總結

```
模型性能排名（RMSE）:
1. XGBoost:        253.96 USDT  ⭐⭐⭐⭐⭐
2. LSTM (簡化):    662.59 USDT  ⭐⭐
3. LSTM (完整):    693.17 USDT  ⭐⭐

模型性能排名（方向準確率）:
1. XGBoost:        86.43%       ⭐⭐⭐⭐⭐
2. LSTM (簡化):    47.41%       ⭐ (接近隨機)
3. LSTM (完整):    46.25%       ⭐ (接近隨機)
```

### 🚀 後續工作建議

1. **XGBoost 優化**：
   - 超參數網格搜索
   - 交叉驗證確保穩定性
   - 添加更多技術指標

2. **策略回測**：
   - 使用 XGBoost 預測構建交易策略
   - 評估真實交易成本下的性能

3. **集成學習**（可選）：
   - XGBoost + LightGBM + CatBoost 集成
   - 可能進一步提升性能

4. **不同時間週期**：
   - 嘗試 4h、日線數據
   - 評估模型在不同市場環境下的穩定性
